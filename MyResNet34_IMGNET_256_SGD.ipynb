{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "####################################################\n",
    "from src.Mydataloader import LoadDataset\n",
    "from src.Mymodel import Block\n",
    "from src.Mymodel import MyResNet34\n",
    "from src.Mymodel import MyResNet_CIFAR\n",
    "from src.Mytraining import DoTraining\n",
    "from src.LogViewer import LogViewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataset selection\"\"\"\n",
    "# DATASET = \"CIFAR10\"\n",
    "# DATASET = \"CIFAR100\"\n",
    "DATASET = \"ImageNet2012\"\n",
    "\n",
    "\"\"\"Model selection for CIFAR\"\"\"\n",
    "NUM_LAYERS_LEVEL = 5\n",
    "\n",
    "\"\"\"Dataset parameters\"\"\"\n",
    "BATCH = 256\n",
    "SHUFFLE = True\n",
    "NUMOFWORKERS = 8\n",
    "PIN_MEMORY = True\n",
    "SPLIT_RATIO = 0\n",
    "\"\"\"optimizer parameters\"\"\"\n",
    "OPTIMIZER = \"SGD\"\n",
    "# OPTIMIZER = \"Adam\"\n",
    "# OPTIMIZER = \"Adam_decay\"\n",
    "\n",
    "\"\"\"Learning rate scheduler parameters\"\"\"\n",
    "LOAD_BEFORE_TRAINING = False\n",
    "LOAD_BEFORE_TRAINING = True\n",
    "NUM_EPOCHS = 50000\n",
    "\n",
    "\"\"\"Early stopping parameters\"\"\"\n",
    "EARLYSTOPPINGPATIENCE = 1200\n",
    "file_path = \"\"\n",
    "if DATASET == \"ImageNet2012\":\n",
    "    file_path = f\"{DATASET}/MyResNet34_{BATCH}_{OPTIMIZER}\"\n",
    "else:\n",
    "    file_path = f\"{DATASET}/MyResNet{NUM_LAYERS_LEVEL*6+2}_{BATCH}_{OPTIMIZER}\"\n",
    "    \n",
    "if SPLIT_RATIO != 0:\n",
    "    file_path += f\"_{int(SPLIT_RATIO*100)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ImageNet2012/MyResNet34_256_SGD'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dateloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lee/anaconda3/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "Dataset :  ImageNet2012\n",
      "- Length of Train Set :  1281167\n",
      "- Count of Classes :  1000\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tmp = LoadDataset(root=\"data\", seceted_dataset=DATASET, split_ratio=SPLIT_RATIO)\n",
    "train_data, valid_data, test_data, COUNT_OF_CLASSES = tmp.Unpack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.transforms = Compose(\n",
      "      ToTensor()\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[1, 1, 1], inplace=True)\n",
      "      RandomShortestSize(min_size=[256], max_size=480, interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
      "      RandomCrop(size=(224, 224), pad_if_needed=False, fill=0, padding_mode=constant)\n",
      "      AutoAugment(interpolation=InterpolationMode.NEAREST, policy=AutoAugmentPolicy.IMAGENET)\n",
      ") 256\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=SHUFFLE,\n",
    "    num_workers=NUMOFWORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    # pin_memory_device=\"cuda\",\n",
    "    persistent_workers=True,\n",
    ")\n",
    "print(\"train.transforms =\", train_data.transform, train_dataloader.batch_size)\n",
    "\n",
    "if valid_data is not None:\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_data,\n",
    "        batch_size=BATCH,\n",
    "        shuffle=SHUFFLE,\n",
    "        num_workers=NUMOFWORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        # pin_memory_device=\"cuda\",\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "    print(\"valid.transforms =\", valid_data.transform, valid_dataloader.batch_size)\n",
    "\n",
    "if test_data is not None:\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=BATCH,\n",
    "        shuffle=SHUFFLE,\n",
    "        num_workers=NUMOFWORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        # pin_memory_device=\"cuda\",\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "    print(\"test.transforms =\", test_data.transform, test_dataloader.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm that the dataset is loaded properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data is not None:\n",
    "    for X, y in test_data:\n",
    "        print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "        print(\"mean of X\", X.mean(dim=(0, 2, 3)))\n",
    "        print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data is not None:\n",
    "    class_names = test_dataloader.dataset.classes\n",
    "    count = 0\n",
    "    fig, axs = plt.subplots(2, 5, figsize=(8, 4))\n",
    "\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.numpy()\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            image = images[i]\n",
    "            label = labels[i]\n",
    "            image = np.transpose(image, (1, 2, 0))\n",
    "            image = np.clip(image, 0, 1)\n",
    "            ax = axs[count // 5, count % 5]\n",
    "            ax.imshow(image)\n",
    "            ax.set_title(f\"{class_names[label], label}\")\n",
    "            ax.axis(\"off\")\n",
    "            count += 1\n",
    "\n",
    "            if count == 10:\n",
    "                break\n",
    "        if count == 10:\n",
    "            break\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-34 for ImageNet2012 is loaded.\n"
     ]
    }
   ],
   "source": [
    "if DATASET == \"CIFAR10\" or DATASET == \"CIFAR100\":\n",
    "    \"\"\"ResNet{20, 32, 44, 56, 110, 1202} for CIFAR\"\"\"\n",
    "    model = MyResNet_CIFAR(\n",
    "        num_classes=COUNT_OF_CLASSES, num_layer_factor=NUM_LAYERS_LEVEL\n",
    "    ).to(device)\n",
    "    print(f\"ResNet-{5*6+2} for {DATASET} is loaded.\")\n",
    "\n",
    "elif DATASET == \"ImageNet2012\":\n",
    "    \"\"\"ResNet34 for ImageNet 2012\"\"\"\n",
    "    model = MyResNet34(num_classes=COUNT_OF_CLASSES).to(device)\n",
    "    # model = models.resnet34(pretrained=True).to(device)\n",
    "    # model = models.resnet34(pretrained=False).to(device)\n",
    "    print(f\"ResNet-34 for {DATASET} is loaded.\")\n",
    "\n",
    "# model.named_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                   | #parameters or shape   | #flops    |\n",
      "|:-------------------------|:-----------------------|:----------|\n",
      "| model                    | 21.624M                | 19.262G   |\n",
      "|  conv1                   |  9.408K                |  0.617G   |\n",
      "|   conv1.weight           |   (64, 3, 7, 7)        |           |\n",
      "|  bn1                     |  0.128K                |  20.972M  |\n",
      "|   bn1.weight             |   (64,)                |           |\n",
      "|   bn1.bias               |   (64,)                |           |\n",
      "|  conv64blocks            |  0.222M                |  3.655G   |\n",
      "|   conv64blocks.0         |   73.984K              |   1.218G  |\n",
      "|    conv64blocks.0.conv1  |    36.864K             |    0.604G |\n",
      "|    conv64blocks.0.bn1    |    0.128K              |    5.243M |\n",
      "|    conv64blocks.0.conv2  |    36.864K             |    0.604G |\n",
      "|    conv64blocks.0.bn2    |    0.128K              |    5.243M |\n",
      "|   conv64blocks.1         |   73.984K              |   1.218G  |\n",
      "|    conv64blocks.1.conv1  |    36.864K             |    0.604G |\n",
      "|    conv64blocks.1.bn1    |    0.128K              |    5.243M |\n",
      "|    conv64blocks.1.conv2  |    36.864K             |    0.604G |\n",
      "|    conv64blocks.1.bn2    |    0.128K              |    5.243M |\n",
      "|   conv64blocks.2         |   73.984K              |   1.218G  |\n",
      "|    conv64blocks.2.conv1  |    36.864K             |    0.604G |\n",
      "|    conv64blocks.2.bn1    |    0.128K              |    5.243M |\n",
      "|    conv64blocks.2.conv2  |    36.864K             |    0.604G |\n",
      "|    conv64blocks.2.bn2    |    0.128K              |    5.243M |\n",
      "|  conv128blocks           |  1.108M                |  4.551G   |\n",
      "|   conv128blocks.0        |   0.222M               |   0.911G  |\n",
      "|    conv128blocks.0.conv1 |    73.728K             |    0.302G |\n",
      "|    conv128blocks.0.bn1   |    0.256K              |    2.621M |\n",
      "|    conv128blocks.0.conv2 |    0.147M              |    0.604G |\n",
      "|    conv128blocks.0.bn2   |    0.256K              |    2.621M |\n",
      "|   conv128blocks.1        |   0.295M               |   1.213G  |\n",
      "|    conv128blocks.1.conv1 |    0.147M              |    0.604G |\n",
      "|    conv128blocks.1.bn1   |    0.256K              |    2.621M |\n",
      "|    conv128blocks.1.conv2 |    0.147M              |    0.604G |\n",
      "|    conv128blocks.1.bn2   |    0.256K              |    2.621M |\n",
      "|   conv128blocks.2        |   0.295M               |   1.213G  |\n",
      "|    conv128blocks.2.conv1 |    0.147M              |    0.604G |\n",
      "|    conv128blocks.2.bn1   |    0.256K              |    2.621M |\n",
      "|    conv128blocks.2.conv2 |    0.147M              |    0.604G |\n",
      "|    conv128blocks.2.bn2   |    0.256K              |    2.621M |\n",
      "|   conv128blocks.3        |   0.295M               |   1.213G  |\n",
      "|    conv128blocks.3.conv1 |    0.147M              |    0.604G |\n",
      "|    conv128blocks.3.bn1   |    0.256K              |    2.621M |\n",
      "|    conv128blocks.3.conv2 |    0.147M              |    0.604G |\n",
      "|    conv128blocks.3.bn2   |    0.256K              |    2.621M |\n",
      "|  conv256blocks           |  6.789M                |  6.961G   |\n",
      "|   conv256blocks.0        |   0.886M               |   0.909G  |\n",
      "|    conv256blocks.0.conv1 |    0.295M              |    0.302G |\n",
      "|    conv256blocks.0.bn1   |    0.512K              |    1.311M |\n",
      "|    conv256blocks.0.conv2 |    0.59M               |    0.604G |\n",
      "|    conv256blocks.0.bn2   |    0.512K              |    1.311M |\n",
      "|   conv256blocks.1        |   1.181M               |   1.211G  |\n",
      "|    conv256blocks.1.conv1 |    0.59M               |    0.604G |\n",
      "|    conv256blocks.1.bn1   |    0.512K              |    1.311M |\n",
      "|    conv256blocks.1.conv2 |    0.59M               |    0.604G |\n",
      "|    conv256blocks.1.bn2   |    0.512K              |    1.311M |\n",
      "|   conv256blocks.2        |   1.181M               |   1.211G  |\n",
      "|    conv256blocks.2.conv1 |    0.59M               |    0.604G |\n",
      "|    conv256blocks.2.bn1   |    0.512K              |    1.311M |\n",
      "|    conv256blocks.2.conv2 |    0.59M               |    0.604G |\n",
      "|    conv256blocks.2.bn2   |    0.512K              |    1.311M |\n",
      "|   conv256blocks.3        |   1.181M               |   1.211G  |\n",
      "|    conv256blocks.3.conv1 |    0.59M               |    0.604G |\n",
      "|    conv256blocks.3.bn1   |    0.512K              |    1.311M |\n",
      "|    conv256blocks.3.conv2 |    0.59M               |    0.604G |\n",
      "|    conv256blocks.3.bn2   |    0.512K              |    1.311M |\n",
      "|   conv256blocks.4        |   1.181M               |   1.211G  |\n",
      "|    conv256blocks.4.conv1 |    0.59M               |    0.604G |\n",
      "|    conv256blocks.4.bn1   |    0.512K              |    1.311M |\n",
      "|    conv256blocks.4.conv2 |    0.59M               |    0.604G |\n",
      "|    conv256blocks.4.bn2   |    0.512K              |    1.311M |\n",
      "|   conv256blocks.5        |   1.181M               |   1.211G  |\n",
      "|    conv256blocks.5.conv1 |    0.59M               |    0.604G |\n",
      "|    conv256blocks.5.bn1   |    0.512K              |    1.311M |\n",
      "|    conv256blocks.5.conv2 |    0.59M               |    0.604G |\n",
      "|    conv256blocks.5.bn2   |    0.512K              |    1.311M |\n",
      "|  conv512blocks           |  12.982M               |  3.326G   |\n",
      "|   conv512blocks.0        |   3.541M               |   0.907G  |\n",
      "|    conv512blocks.0.conv1 |    1.18M               |    0.302G |\n",
      "|    conv512blocks.0.bn1   |    1.024K              |    0.655M |\n",
      "|    conv512blocks.0.conv2 |    2.359M              |    0.604G |\n",
      "|    conv512blocks.0.bn2   |    1.024K              |    0.655M |\n",
      "|   conv512blocks.1        |   4.721M               |   1.209G  |\n",
      "|    conv512blocks.1.conv1 |    2.359M              |    0.604G |\n",
      "|    conv512blocks.1.bn1   |    1.024K              |    0.655M |\n",
      "|    conv512blocks.1.conv2 |    2.359M              |    0.604G |\n",
      "|    conv512blocks.1.bn2   |    1.024K              |    0.655M |\n",
      "|   conv512blocks.2        |   4.721M               |   1.209G  |\n",
      "|    conv512blocks.2.conv1 |    2.359M              |    0.604G |\n",
      "|    conv512blocks.2.bn1   |    1.024K              |    0.655M |\n",
      "|    conv512blocks.2.conv2 |    2.359M              |    0.604G |\n",
      "|    conv512blocks.2.bn2   |    1.024K              |    0.655M |\n",
      "|  fc1                     |  0.513M                |  0.131G   |\n",
      "|   fc1.weight             |   (1000, 512)          |           |\n",
      "|   fc1.bias               |   (1000,)              |           |\n",
      "|  avgpool                 |                        |  0.131M   |\n"
     ]
    }
   ],
   "source": [
    "tmp_input = torch.rand(BATCH, 3, 32, 32).to(device)\n",
    "flops = FlopCountAnalysis(model, tmp_input)\n",
    "print(flop_count_table(flops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Define Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Define Optimazer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMIZER == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "elif OPTIMIZER == \"Adam_decay\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "elif OPTIMIZER == \"SGD\":\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Define Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience, model, file_path):\n",
    "        self.best_eval_loss = float(\"inf\")\n",
    "        self.early_stop_counter = 0\n",
    "        self.PATIENCE = patience\n",
    "        self.file_path = file_path\n",
    "        self.model = model\n",
    "        pass\n",
    "\n",
    "    def check(self, eval_loss):\n",
    "        if eval_loss < self.best_eval_loss:\n",
    "            self.best_eval_loss = eval_loss\n",
    "            self.early_stop_counter = 0\n",
    "            print(\"updated best eval loss :\", self.best_eval_loss)\n",
    "            torch.save(self.model.state_dict(), \"models/\" + self.file_path + \".pth\")\n",
    "        else:\n",
    "            self.early_stop_counter += 1\n",
    "            if self.early_stop_counter >= self.PATIENCE:\n",
    "                print(f\"Early stop!! best_eval_loss = {self.best_eval_loss}\")\n",
    "                \n",
    "    def state_dict(self):\n",
    "        return {\"best_eval_loss\": self.best_eval_loss, \"early_stop_counter\": self.early_stop_counter}\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.best_eval_loss = state_dict[\"best_eval_loss\"]\n",
    "        self.early_stop_counter = state_dict[\"early_stop_counter\"]\n",
    "        \n",
    "        return\n",
    "    \n",
    "earlystopper = EarlyStopper(EARLYSTOPPINGPATIENCE, model, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Define Learning Rate schedualer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_mapping = {\"CIFAR100\": 30, \"CIFAR10\": 30, \"ImageNet2012\": 30}\n",
    "MIN_LR = 0.0001\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    patience=scheduler_mapping[DATASET],\n",
    "    factor=0.1,\n",
    "    verbose=True,\n",
    "    threshold=1e-4,\n",
    "    min_lr=MIN_LR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Define AMP scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load before process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist. Created a new log.\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "if LOAD_BEFORE_TRAINING == True and os.path.exists(\"logs/\" + file_path + \".pth.tar\"):\n",
    "    # Read checkpoint as desired, e.g.,\n",
    "    checkpoint = torch.load(\n",
    "        \"logs/\" + file_path + \".pth.tar\",\n",
    "        map_location=lambda storage, loc: storage.cuda(device),\n",
    "    )\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    scaler.load_state_dict(checkpoint[\"scaler\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
    "    earlystopper.load_state_dict(checkpoint[\"earlystopper\"])\n",
    "    logs = checkpoint[\"logs\"]\n",
    "\n",
    "    print(\"Suceessfully loaded the All setting and Log file.\")\n",
    "    print(file_path)\n",
    "    print(f\"Current epoch is {len(logs['train_loss'])}\")\n",
    "    print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "else:\n",
    "    # Create a dictionary to store the variables\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    eval_loss = []\n",
    "    valid_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    lr_log = []\n",
    "    logs = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"valid_loss\": eval_loss,\n",
    "        \"valid_acc\": valid_acc,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"lr_log\": lr_log,\n",
    "    }\n",
    "    print(\"File does not exist. Created a new log.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK = 5410\n",
    "# logs[\"train_loss\"] = logs[\"train_loss\"][:CHECK]\n",
    "# logs[\"train_acc\"] = logs[\"train_acc\"][:CHECK]\n",
    "# logs[\"valid_loss\"] = logs[\"valid_loss\"][:CHECK]\n",
    "# logs[\"valid_acc\"] = logs[\"valid_acc\"][:CHECK]\n",
    "# logs[\"test_loss\"] = logs[\"test_loss\"][:CHECK]\n",
    "# logs[\"test_acc\"] = logs[\"test_acc\"][:CHECK]\n",
    "# model.load_state_dict(torch.load(f\"models/{file_path}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler.patience 수정하는 곳.\n",
    "# scheduler.patience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Training Loop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if valid_data is None:\n",
    "    valid_dataloader = None\n",
    "if test_data is None:\n",
    "    test_dataloader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/50000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5005 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/5005 [00:05<7:41:47,  5.54s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torchvision/datasets/folder.py\", line 231, in __getitem__\n    sample = self.transform(sample)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torchvision/transforms/v2/_container.py\", line 53, in forward\n    outputs = transform(*inputs)\n              ^^^^^^^^^^^^^^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py\", line 46, in forward\n    params = self._get_params(\n             ^^^^^^^^^^^^^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torchvision/transforms/v2/_geometry.py\", line 889, in _get_params\n    raise ValueError(\nValueError: Required crop size (224, 224) is larger than input image size (181, 480).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mpre_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] :\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DATASET \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImageNet2012\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     eval_loss \u001b[38;5;241m=\u001b[39m Training\u001b[38;5;241m.\u001b[39mSingleEpoch(train_dataloader, train_dataloader)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     eval_loss \u001b[38;5;241m=\u001b[39m Training\u001b[38;5;241m.\u001b[39mSingleEpoch(\n\u001b[1;32m     22\u001b[0m         train_dataloader, valid_dataloader, test_dataloader\n\u001b[1;32m     23\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Learn_pytorch/src/Mytraining.py:101\u001b[0m, in \u001b[0;36mDoTraining.SingleEpoch\u001b[0;34m(self, train_dataloader, valid_dataloader, test_dataloader)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid/test dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_log\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 101\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mForward_train(train_dataloader)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_acc)\n",
      "File \u001b[0;32m~/Desktop/Learn_pytorch/src/Mytraining.py:34\u001b[0m, in \u001b[0;36mDoTraining.Forward_train\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     32\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     33\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(dataset):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16, enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     36\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1325\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1324\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torchvision/datasets/folder.py\", line 231, in __getitem__\n    sample = self.transform(sample)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torchvision/transforms/v2/_container.py\", line 53, in forward\n    outputs = transform(*inputs)\n              ^^^^^^^^^^^^^^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py\", line 46, in forward\n    params = self._get_params(\n             ^^^^^^^^^^^^^^^^^\n  File \"/home/lee/anaconda3/lib/python3.11/site-packages/torchvision/transforms/v2/_geometry.py\", line 889, in _get_params\n    raise ValueError(\nValueError: Required crop size (224, 224) is larger than input image size (181, 480).\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "Training = DoTraining(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    scheduler=scheduler,\n",
    "    earlystopper=earlystopper,\n",
    "    device=device,\n",
    "    logs=logs,\n",
    "    file_path=file_path,\n",
    ")\n",
    "pre_epochs = len(Training.logs[\"train_loss\"])\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    now = epoch + 1 + pre_epochs\n",
    "    print(f\"[Epoch {epoch+1+pre_epochs}/{NUM_EPOCHS}] :\")\n",
    "\n",
    "    if DATASET == \"ImageNet2012\":\n",
    "        eval_loss = Training.SingleEpoch(train_dataloader, train_dataloader)\n",
    "    else:\n",
    "        eval_loss = Training.SingleEpoch(\n",
    "            train_dataloader, valid_dataloader, test_dataloader\n",
    "        )\n",
    "\n",
    "    Training.Save()\n",
    "\n",
    "    if earlystopper.check(eval_loss) == True:\n",
    "        break\n",
    "\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = LogViewer(logs)\n",
    "view.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view.print_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
