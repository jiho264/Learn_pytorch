{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 공개 데이터셋에서 학습 데이터를 내려받습니다.\n",
    "training_data = datasets.CIFAR100(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# 공개 데이터셋에서 테스트 데이터를 내려받습니다.\n",
    "test_data = datasets.CIFAR100(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nResNet paper :\\nOur implementation for ImageNet follows the practice in [21, 41]. \\nThe image is resized with its shorter side ran- domly sampled in [256, 480] for scale augmentation [41]. \\nA 224×224 crop is randomly sampled from an image or its horizontal flip, with the per-pixel mean subtracted [21]. \\nThe standard color augmentation in [21] is used\\n\\n[21] : AlexNet - Dataset section\\nWe did not pre-process the images in any other way, except for subtracting the mean activity over the training set from each pixel. \\nSo we trained our network on the (centered) raw RGB values of the pixels.\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ResNet paper :\n",
    "Our implementation for ImageNet follows the practice in [21, 41]. \n",
    "The image is resized with its shorter side ran- domly sampled in [256, 480] for scale augmentation [41]. \n",
    "A 224×224 crop is randomly sampled from an image or its horizontal flip, with the per-pixel mean subtracted [21]. \n",
    "The standard color augmentation in [21] is used\n",
    "\n",
    "[21] : AlexNet - Dataset section\n",
    "We did not pre-process the images in any other way, except for subtracting the mean activity over the training set from each pixel. \n",
    "So we trained our network on the (centered) raw RGB values of the pixels.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "\n",
    "print(training_data.data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([256, 3, 32, 32])\n",
      "Shape of y: torch.Size([256]) torch.int64\n",
      "['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "print(test_dataloader.dataset.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# 학습에 사용할 CPU나 GPU, MPS 장치를 얻습니다.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    - 왜 블럭 하나엔 relu가 하나만 있을까?\\n    \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, inputs, outputs, _Downsample = False):\n",
    "        super().__init__()\n",
    "        self._Downsample = _Downsample\n",
    "\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(inputs, outputs, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(outputs, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(outputs, outputs, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(outputs, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        #self.relu2 = nn.ReLU(inplace=False)\n",
    "        \n",
    "        if self._Downsample == True:\n",
    "            self.downsampling = nn.Sequential(\n",
    "                # In paper, \n",
    "                # downsampling is performed by conv3_1, conv4_1, and conv5_1 with a stride of 2\n",
    "                nn.Conv2d(inputs, outputs, kernel_size=1, stride=2, bias=False),\n",
    "                nn.BatchNorm2d(outputs, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "                # 이거 토치 공식 resnet에서도 배치까지 있는 4차원이라 conv2d로 함.\n",
    "            )\n",
    "            \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if m == self.conv1 and self._Downsample == True:\n",
    "                    m.stride = (2, 2)\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print('x1(identity) :', x.shape, 'Downsample :', self._Downsample)\n",
    "        identity = x\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        # print('x2 :', x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        if self._Downsample == True:\n",
    "            identity = self.downsampling(identity)\n",
    "            # print('x1(downsampled) :', identity.shape)\n",
    "        x = x + identity # 여기 x+=identity로 하면 안됨. inplace operation이라서.\n",
    "        \n",
    "        #x = self.relu2(x)\n",
    "        # print('x3 :', x.shape)\n",
    "        # print('x4 :', x.shape)\n",
    "        return x\n",
    "    \n",
    "class MyResNet34(nn.Module):\n",
    "    def __init__(self, _BlockClass = Block, _num_classes = 1000, _print = False):\n",
    "        super().__init__()\n",
    "        self._num_classes = _num_classes\n",
    "        self._print = _print\n",
    "        \n",
    "        # 1. input layer\n",
    "        # input : 224x224x3\n",
    "        # output : 112x112x64\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels= 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "        \n",
    "        # 2. 64ch 3x3 conv x 3\n",
    "        # input : 112x112x64\n",
    "        # output : 56x56x64\n",
    "        self.conv64blocks = nn.Sequential(\n",
    "            _BlockClass(64, 64),\n",
    "            _BlockClass(64, 64),\n",
    "            _BlockClass(64, 64)\n",
    "        )\n",
    "        # 3. 128ch 3x3 conv x 4\n",
    "        # input : 56x56x64\n",
    "        # output : 28x28x128\n",
    "        self.conv128blocks = nn.Sequential(\n",
    "            _BlockClass(64, 128, _Downsample = True),\n",
    "            _BlockClass(128, 128),\n",
    "            _BlockClass(128, 128),\n",
    "            _BlockClass(128, 128)\n",
    "        )\n",
    "        # 4. 256ch 3x3 conv x 6\n",
    "        # input : 28x28x128\n",
    "        # output : 14x14x256\n",
    "        self.conv256blocks = nn.Sequential(\n",
    "            _BlockClass(128, 256, _Downsample = True),\n",
    "            _BlockClass(256, 256),\n",
    "            _BlockClass(256, 256),\n",
    "            _BlockClass(256, 256),\n",
    "            _BlockClass(256, 256),\n",
    "            _BlockClass(256, 256)\n",
    "        )\n",
    "        # 5. 512ch 3x3 conv x 3\n",
    "        # input : 14x14x256\n",
    "        # output : 7x7x512\n",
    "        self.conv512blocks = nn.Sequential(\n",
    "            _BlockClass(256, 512, _Downsample = True),\n",
    "            _BlockClass(512, 512),\n",
    "            _BlockClass(512, 512)\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(in_features = 512, out_features= self._num_classes, bias=True)\n",
    "        #self.relu2 = nn.ReLU(inplace=False)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        if(self._print): print('input 64ch :', x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        if(self._print): print('conv7x7 64ch :', x.shape)\n",
    "        x = self.maxpool(x)\n",
    "        if(self._print): print('maxpool 64ch :', x.shape)\n",
    "        \n",
    "        x = self.conv64blocks(x)\n",
    "        if(self._print): print('1st conv 64 :', x.shape)\n",
    "        \n",
    "        x = self.conv128blocks(x)\n",
    "        if(self._print): print('2nd conv 128 :', x.shape)\n",
    "        \n",
    "        x = self.conv256blocks(x)\n",
    "        if(self._print): print('3rd conv 256 :', x.shape)\n",
    "        \n",
    "        x = self.conv512blocks(x)\n",
    "        if(self._print): print('4th conv 512:', x.shape)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        if(self._print): print('avgpool :', x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        #x = self.relu2(x)\n",
    "        if(self._print): print('fc :', x.shape)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\"\"\"\n",
    "    - 왜 블럭 하나엔 relu가 하나만 있을까?\n",
    "    \n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # tensor = torch.randn(64, 3, 224, 224)\n",
    "# tensor = torch.randn(64, 3, 32, 32)\n",
    "# print(tensor.shape)\n",
    "# out = model(tensor.to(device))\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(out.shape)\n",
    "# print(torch.argmax(out, dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_modules of MyResNet34(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv64blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (conv128blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsampling): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (conv256blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsampling): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (conv512blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsampling): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=100, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# for imagenet\n",
    "# model = MyResNet34(Block).to(device)\n",
    "\n",
    "# for cifar100\n",
    "model = MyResNet34(Block, 100, _print=False).to(device)\n",
    "model.named_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    we initialize the weights as on He initialization\\n    we adopt batch normalization after each convolutional and before activation\\n    we use SGD with a mini-batch size of 256\\n    the learning rate starts from 0.1 and is divided by 10 when the error plateaus\\n    we use a weight decay of 0.0001 and a momentum of 0.9\\n    we do not use dropout\\n    \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    we initialize the weights as on He initialization\n",
    "    we adopt batch normalization after each convolutional and before activation\n",
    "    we use SGD with a mini-batch size of 256\n",
    "    the learning rate starts from 0.1 and is divided by 10 when the error plateaus\n",
    "    we use a weight decay of 0.0001 and a momentum of 0.9\n",
    "    we do not use dropout\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성 및 손실 함수, 최적화기 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\"\"\"\n",
    "    It is useful when training a classification problem with C classes. \n",
    "    If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. \n",
    "    This is particularly useful when you have an unbalanced training set.\n",
    "    \n",
    "    The input is expected to contain the unnormalized logits for each class (which do not need to be positive or sum to 1, in general).\n",
    "\"\"\"\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=0.0001, momentum=0.9)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Train Loss: 3.5694 | Train Acc: 17.59%\n",
      "Test Loss: 3.2424 | Test Acc: 22.41%\n",
      "--------------------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Loss: 2.7602 | Train Acc: 30.89%\n",
      "Test Loss: 2.9852 | Test Acc: 27.67%\n",
      "--------------------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Loss: 2.3288 | Train Acc: 39.53%\n",
      "Test Loss: 3.1244 | Test Acc: 26.30%\n",
      "--------------------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Loss: 2.0154 | Train Acc: 46.29%\n",
      "Test Loss: 3.7881 | Test Acc: 20.26%\n",
      "--------------------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Loss: 1.7692 | Train Acc: 51.46%\n",
      "Test Loss: 4.4719 | Test Acc: 17.20%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "_log_train_loss = []\n",
    "_log_train_acc = []\n",
    "_log_test_loss = []\n",
    "_log_test_acc = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    #for inputs, labels in tqdm.tqdm(train_dataloader):\n",
    "    for images, labels in train_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_dataloader)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc = correct / total\n",
    "\n",
    "    # Print epoch statistics\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "    print(\"-\" * 50)\n",
    "    _log_train_loss.append(train_loss)\n",
    "    _log_train_acc.append(train_acc)\n",
    "    _log_test_loss.append(test_loss)\n",
    "    _log_test_acc.append(test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    - 왜 downsampling된 블럭에선 stride=2인가?\n",
    "    - 왜 batchnorm에서 eps를 비롯한 옵션들의 설정 추가가 유효했는가? 기존엔 #value만 적었었음.\n",
    "    - 왜 Adam에 LR 0.1을 주면 학습이 안되는가?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training loss and accuracy\n",
    "plt.plot(_log_train_loss, label='Training Loss')\n",
    "plt.plot(_log_test_loss, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(_log_test_acc, label='Test Accuracy')\n",
    "plt.plot(_log_train_acc, label='Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"Myresnet34.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyResNet34(Block, 100).to(device)\n",
    "model.load_state_dict(torch.load(\"Myresnet34.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
