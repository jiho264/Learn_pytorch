{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "####################################################\n",
    "from src.Mydataloader import LoadDataset\n",
    "from src.Mymodel import MyResNet34\n",
    "from src.Mymodel import MyResNet_CIFAR\n",
    "from src.Mytraining import DoTraining\n",
    "from src.LogViewer import LogViewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataset selection\"\"\"\n",
    "DATASET = \"CIFAR10\"\n",
    "# DATASET = \"CIFAR100\"\n",
    "# DATASET = \"ImageNet2012\"\n",
    "\n",
    "\"\"\"Model selection for CIFAR\"\"\"\n",
    "NUM_LAYERS_LEVEL = 5\n",
    "\n",
    "\"\"\"Dataset parameters\"\"\"\n",
    "BATCH = 256\n",
    "SHUFFLE = True\n",
    "NUMOFWORKERS = 8\n",
    "PIN_MEMORY = True\n",
    "SPLIT_RATIO = 0.9\n",
    "\"\"\"optimizer parameters\"\"\"\n",
    "OPTIMIZER = \"SGD\"\n",
    "# OPTIMIZER = \"Adam\"\n",
    "# OPTIMIZER = \"Adam_decay\"\n",
    "\n",
    "\"\"\"Learning rate scheduler parameters\"\"\"\n",
    "# LOAD_BEFORE_TRAINING = False\n",
    "LOAD_BEFORE_TRAINING = True\n",
    "NUM_EPOCHS = 100000\n",
    "\n",
    "\"\"\"Early stopping parameters\"\"\"\n",
    "EARLYSTOPPINGPATIENCE = 3000\n",
    "file_path = \"\"\n",
    "if DATASET == \"ImageNet2012\":\n",
    "    file_path = f\"{DATASET}/MyResNet34_{BATCH}_{OPTIMIZER}\"\n",
    "else:\n",
    "    file_path = f\"{DATASET}/MyResNet{NUM_LAYERS_LEVEL*6+2}_{BATCH}_{OPTIMIZER}\"\n",
    "    \n",
    "if SPLIT_RATIO != 0:\n",
    "    file_path += f\"_{int(SPLIT_RATIO*100)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dateloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = LoadDataset(root=\"data\", seceted_dataset=DATASET, split_ratio=SPLIT_RATIO)\n",
    "train_data, valid_data, test_data, COUNT_OF_CLASSES = tmp.Unpack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=SHUFFLE,\n",
    "    num_workers=NUMOFWORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    # pin_memory_device=\"cuda\",\n",
    "    persistent_workers=True,\n",
    ")\n",
    "print(\"train.transforms =\", train_data.transform, train_dataloader.batch_size)\n",
    "\n",
    "if SPLIT_RATIO != 0:\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_data,\n",
    "        batch_size=BATCH,\n",
    "        shuffle=SHUFFLE,\n",
    "        num_workers=NUMOFWORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        # pin_memory_device=\"cuda\",\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "    print(\"valid.transforms =\", valid_data.transform, valid_dataloader.batch_size)\n",
    "elif SPLIT_RATIO == 0:\n",
    "    valid_dataloader = None\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=SHUFFLE,\n",
    "    num_workers=NUMOFWORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    # pin_memory_device=\"cuda\",\n",
    "    persistent_workers=True,\n",
    ")\n",
    "print(\"test.transforms =\", test_data.transform, test_dataloader.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm that the dataset is loaded properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET != \"ImageNet2012\":\n",
    "    for X, y in test_dataloader:\n",
    "        print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "        print(\"mean of X\", X.mean(dim=(0, 2, 3)))\n",
    "        print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET != \"ImageNet2012\":\n",
    "    class_names = test_dataloader.dataset.classes\n",
    "    count = 0\n",
    "    fig, axs = plt.subplots(2, 5, figsize=(8, 4))\n",
    "\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.numpy()\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            image = images[i]\n",
    "            label = labels[i]\n",
    "            image = np.transpose(image, (1, 2, 0))\n",
    "            image = np.clip(image, 0, 1)\n",
    "            ax = axs[count // 5, count % 5]\n",
    "            ax.imshow(image)\n",
    "            ax.set_title(f\"{class_names[label], label}\")\n",
    "            ax.axis('off')\n",
    "            count += 1\n",
    "            \n",
    "            if count == 10:\n",
    "                break\n",
    "        if count == 10:\n",
    "            break\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"CIFAR10\" or DATASET == \"CIFAR100\":\n",
    "    \"\"\"ResNet{20, 32, 44, 56, 110, 1202} for CIFAR\"\"\"\n",
    "    model = MyResNet_CIFAR(\n",
    "        num_classes=COUNT_OF_CLASSES,\n",
    "        num_layer_factor=NUM_LAYERS_LEVEL,\n",
    "        Downsample_option=\"A\",\n",
    "    ).to(device)\n",
    "    print(f\"ResNet-{5*6+2} for {DATASET} is loaded.\")\n",
    "\n",
    "elif DATASET == \"ImageNet2012\":\n",
    "    \"\"\"ResNet34 for ImageNet 2012\"\"\"\n",
    "    model = MyResNet34(\n",
    "        num_classes=COUNT_OF_CLASSES, \n",
    "        Downsample_option=\"A\"\n",
    "    ).to(device)\n",
    "    # model = models.resnet34(pretrained=True).to(device)\n",
    "    # model = models.resnet34(pretrained=False).to(device)\n",
    "    print(f\"ResNet-34 for {DATASET} is loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_input = torch.rand(BATCH, 3, 32, 32).to(device)\n",
    "flops = FlopCountAnalysis(model, tmp_input)\n",
    "print(flop_count_table(flops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Define Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Define Optimazer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMIZER == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "elif OPTIMIZER == \"Adam_decay\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "elif OPTIMIZER == \"SGD\":\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Define Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience, model, file_path):\n",
    "        self.best_eval_loss = float(\"inf\")\n",
    "        self.early_stop_counter = 0\n",
    "        self.PATIENCE = patience\n",
    "        self.file_path = file_path\n",
    "        self.model = model\n",
    "        pass\n",
    "\n",
    "    def check(self, eval_loss):\n",
    "        if eval_loss < self.best_eval_loss:\n",
    "            self.best_eval_loss = eval_loss\n",
    "            self.early_stop_counter = 0\n",
    "            print(\"updated best eval loss :\", self.best_eval_loss)\n",
    "            torch.save(self.model.state_dict(), \"models/\" + self.file_path + \".pth\")\n",
    "            return False\n",
    "        else:\n",
    "            self.early_stop_counter += 1\n",
    "            if self.early_stop_counter >= self.PATIENCE:\n",
    "                print(f\"Early stop!! best_eval_loss = {self.best_eval_loss}\")\n",
    "                return True\n",
    "                \n",
    "    def state_dict(self):\n",
    "        return {\"best_eval_loss\": self.best_eval_loss, \"early_stop_counter\": self.early_stop_counter}\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.best_eval_loss = state_dict[\"best_eval_loss\"]\n",
    "        self.early_stop_counter = state_dict[\"early_stop_counter\"]\n",
    "        \n",
    "        return\n",
    "    \n",
    "earlystopper = EarlyStopper(EARLYSTOPPINGPATIENCE, model, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Define Learning Rate schedualer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_mapping = {\"CIFAR10\": 1000, \"CIFAR100\": 1000, \"ImageNet2012\": 30}\n",
    "\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    patience=scheduler_mapping[DATASET],\n",
    "    factor=0.1,\n",
    "    verbose=True,\n",
    "    threshold=1e-4,\n",
    "    cooldown=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Define AMP scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load before process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "if LOAD_BEFORE_TRAINING == True and os.path.exists(\"logs/\" + file_path + \".pth.tar\"):\n",
    "    # Read checkpoint as desired, e.g.,\n",
    "    checkpoint = torch.load(\n",
    "        \"logs/\" + file_path + \".pth.tar\",\n",
    "        map_location=lambda storage, loc: storage.cuda(device),\n",
    "    )\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    scaler.load_state_dict(checkpoint[\"scaler\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
    "    earlystopper.load_state_dict(checkpoint[\"earlystopper\"])\n",
    "    logs = checkpoint[\"logs\"]\n",
    "\n",
    "    print(\"Suceessfully loaded the All setting and Log file.\")\n",
    "    print(file_path)\n",
    "    print(f\"Current epoch is {len(logs['train_loss'])}\")\n",
    "    print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "else:\n",
    "    # Create a dictionary to store the variables\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    eval_loss = []\n",
    "    valid_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    lr_log = []\n",
    "    logs = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"valid_loss\": eval_loss,\n",
    "        \"valid_acc\": valid_acc,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"lr_log\": lr_log,\n",
    "    }\n",
    "    print(\"File does not exist. Created a new log.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0][\"lr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Training Loop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 19.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0024 | Train Acc: 87.00%\n",
      "Valid Loss: 0.6854 | Valid Acc: 77.76%\n",
      "Test  Loss: 0.4705 | Test Acc: 85.49%\n",
      "--------------------------------------------------\n",
      "[Epoch 317/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:09<00:00, 18.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0024 | Train Acc: 85.00%\n",
      "Valid Loss: 0.6161 | Valid Acc: 79.54%\n",
      "Test  Loss: 0.3714 | Test Acc: 88.28%\n",
      "--------------------------------------------------\n",
      "[Epoch 318/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 19.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0034 | Train Acc: 78.50%\n",
      "Valid Loss: 0.6072 | Valid Acc: 79.36%\n",
      "Test  Loss: 0.3768 | Test Acc: 87.96%\n",
      "--------------------------------------------------\n",
      "[Epoch 319/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 19.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0029 | Train Acc: 82.00%\n",
      "Valid Loss: 0.7127 | Valid Acc: 76.94%\n",
      "Test  Loss: 0.5086 | Test Acc: 84.11%\n",
      "--------------------------------------------------\n",
      "[Epoch 320/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:09<00:00, 19.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0025 | Train Acc: 87.00%\n",
      "Valid Loss: 0.7014 | Valid Acc: 77.44%\n",
      "Test  Loss: 0.4609 | Test Acc: 86.59%\n",
      "--------------------------------------------------\n",
      "[Epoch 321/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 20.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0023 | Train Acc: 86.00%\n",
      "Valid Loss: 0.6106 | Valid Acc: 80.28%\n",
      "Test  Loss: 0.3617 | Test Acc: 88.20%\n",
      "--------------------------------------------------\n",
      "[Epoch 322/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 20.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0018 | Train Acc: 91.00%\n",
      "Valid Loss: 0.5935 | Valid Acc: 80.06%\n",
      "Test  Loss: 0.3902 | Test Acc: 87.37%\n",
      "--------------------------------------------------\n",
      "[Epoch 323/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 20.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0024 | Train Acc: 84.00%\n",
      "Valid Loss: 0.5371 | Valid Acc: 81.84%\n",
      "Test  Loss: 0.3316 | Test Acc: 89.28%\n",
      "updated best eval loss : 0.5370827436447143\n",
      "--------------------------------------------------\n",
      "[Epoch 324/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 19.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0028 | Train Acc: 84.00%\n",
      "Valid Loss: 0.5715 | Valid Acc: 81.14%\n",
      "Test  Loss: 0.3053 | Test Acc: 89.59%\n",
      "--------------------------------------------------\n",
      "[Epoch 325/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:10<00:00, 17.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0022 | Train Acc: 87.50%\n",
      "Valid Loss: 0.6461 | Valid Acc: 78.96%\n",
      "Test  Loss: 0.3596 | Test Acc: 89.07%\n",
      "--------------------------------------------------\n",
      "[Epoch 326/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:09<00:00, 19.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0022 | Train Acc: 87.00%\n",
      "Valid Loss: 0.5946 | Valid Acc: 80.12%\n",
      "Test  Loss: 0.3506 | Test Acc: 89.10%\n",
      "--------------------------------------------------\n",
      "[Epoch 327/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:09<00:00, 19.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0030 | Train Acc: 82.50%\n",
      "Valid Loss: 0.5989 | Valid Acc: 80.02%\n",
      "Test  Loss: 0.3794 | Test Acc: 88.06%\n",
      "--------------------------------------------------\n",
      "[Epoch 328/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 20.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0026 | Train Acc: 85.00%\n",
      "Valid Loss: 0.5746 | Valid Acc: 80.82%\n",
      "Test  Loss: 0.3569 | Test Acc: 88.51%\n",
      "--------------------------------------------------\n",
      "[Epoch 329/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 19.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0016 | Train Acc: 89.50%\n",
      "Valid Loss: 0.6516 | Valid Acc: 77.80%\n",
      "Test  Loss: 0.4035 | Test Acc: 87.70%\n",
      "--------------------------------------------------\n",
      "[Epoch 330/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 20.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0020 | Train Acc: 87.00%\n",
      "Valid Loss: 0.6126 | Valid Acc: 79.86%\n",
      "Test  Loss: 0.3504 | Test Acc: 88.52%\n",
      "--------------------------------------------------\n",
      "[Epoch 331/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 20.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0025 | Train Acc: 83.50%\n",
      "Valid Loss: 0.6582 | Valid Acc: 78.44%\n",
      "Test  Loss: 0.4586 | Test Acc: 84.98%\n",
      "--------------------------------------------------\n",
      "[Epoch 332/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 20.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0026 | Train Acc: 82.50%\n",
      "Valid Loss: 0.6263 | Valid Acc: 78.92%\n",
      "Test  Loss: 0.3533 | Test Acc: 89.11%\n",
      "--------------------------------------------------\n",
      "[Epoch 333/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 20.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0020 | Train Acc: 86.50%\n",
      "Valid Loss: 0.5544 | Valid Acc: 81.26%\n",
      "Test  Loss: 0.3776 | Test Acc: 88.06%\n",
      "--------------------------------------------------\n",
      "[Epoch 334/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 20.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0035 | Train Acc: 79.00%\n",
      "Valid Loss: 0.5757 | Valid Acc: 80.38%\n",
      "Test  Loss: 0.3238 | Test Acc: 89.41%\n",
      "--------------------------------------------------\n",
      "[Epoch 335/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 20.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0023 | Train Acc: 84.50%\n",
      "Valid Loss: 0.5685 | Valid Acc: 81.16%\n",
      "Test  Loss: 0.3135 | Test Acc: 90.16%\n",
      "--------------------------------------------------\n",
      "[Epoch 336/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 20.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0025 | Train Acc: 83.00%\n",
      "Valid Loss: 0.5268 | Valid Acc: 81.98%\n",
      "Test  Loss: 0.3285 | Test Acc: 89.63%\n",
      "updated best eval loss : 0.5268193453550338\n",
      "--------------------------------------------------\n",
      "[Epoch 337/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 20.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0018 | Train Acc: 88.50%\n",
      "Valid Loss: 0.6527 | Valid Acc: 78.30%\n",
      "Test  Loss: 0.3927 | Test Acc: 87.59%\n",
      "--------------------------------------------------\n",
      "[Epoch 338/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 19.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0027 | Train Acc: 82.50%\n",
      "Valid Loss: 0.6090 | Valid Acc: 79.50%\n",
      "Test  Loss: 0.3619 | Test Acc: 88.13%\n",
      "--------------------------------------------------\n",
      "[Epoch 339/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 19.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0026 | Train Acc: 83.50%\n",
      "Valid Loss: 0.5996 | Valid Acc: 80.10%\n",
      "Test  Loss: 0.3484 | Test Acc: 89.17%\n",
      "--------------------------------------------------\n",
      "[Epoch 340/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:08<00:00, 20.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0019 | Train Acc: 91.50%\n",
      "Valid Loss: 0.6078 | Valid Acc: 79.38%\n",
      "Test  Loss: 0.3625 | Test Acc: 88.26%\n",
      "--------------------------------------------------\n",
      "[Epoch 341/100000] :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:09<00:00, 18.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0023 | Train Acc: 84.50%\n",
      "Valid Loss: 0.5577 | Valid Acc: 81.42%\n"
     ]
    }
   ],
   "source": [
    "Training = DoTraining(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    scheduler=scheduler,\n",
    "    earlystopper=earlystopper,\n",
    "    device=device,\n",
    "    logs=logs,\n",
    "    file_path=file_path,\n",
    ")\n",
    "pre_epochs = len(Training.logs[\"train_loss\"])\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    now = epoch + 1 + pre_epochs\n",
    "    print(f\"[Epoch {epoch+1+pre_epochs}/{NUM_EPOCHS}] :\")\n",
    "\n",
    "    if DATASET == \"ImageNet2012\":\n",
    "        eval_loss = Training.SingleEpoch(train_dataloader, valid_dataloader)\n",
    "    else:\n",
    "        eval_loss = Training.SingleEpoch(\n",
    "            train_dataloader, valid_dataloader, test_dataloader\n",
    "        )\n",
    "\n",
    "    Training.Save()\n",
    "\n",
    "    if earlystopper.check(eval_loss) == True:\n",
    "        break\n",
    "\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = LogViewer(logs)\n",
    "view.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view.print_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK = 5410\n",
    "# logs[\"train_loss\"] = logs[\"train_loss\"][:CHECK]\n",
    "# logs[\"train_acc\"] = logs[\"train_acc\"][:CHECK]\n",
    "# logs[\"valid_loss\"] = logs[\"valid_loss\"][:CHECK]\n",
    "# logs[\"valid_acc\"] = logs[\"valid_acc\"][:CHECK]\n",
    "# logs[\"test_loss\"] = logs[\"test_loss\"][:CHECK]\n",
    "# logs[\"test_acc\"] = logs[\"test_acc\"][:CHECK]\n",
    "# model.load_state_dict(torch.load(f\"models/{file_path}.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
