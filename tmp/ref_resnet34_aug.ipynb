{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "SubtractMean_and_HorizontalFlip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lee/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/lee/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet34\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import ConcatDataset, ChainDataset\n",
    "from torchvision.transforms import RandomHorizontalFlip, Compose\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class Submean(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        return None\n",
    "    \"\"\"\n",
    "    def __call__(self, tensor):\n",
    "    def forward(self, tensor):\n",
    "    둘 다 가능.\n",
    "    근데, 처음에 한 번 적용되고 끝이 아니라\n",
    "    데이터셋에 접근 할 때마다 계속 적용되는 것 같음.\n",
    "    \"\"\"\n",
    "    def __call__(self, tensor):\n",
    "        # Subtract the mean from each pixel along each channel\n",
    "        # print(\"input\", tensor.shape)\n",
    "        _mean = tensor.mean(axis=(1, 2))\n",
    "        # print(\"mean value before : \", _mean)\n",
    "        tensor = tensor - _mean[:, None, None]\n",
    "        # print(\"mean value after : \", tensor.mean(axis=(1, 2)))\n",
    "        \n",
    "        return tensor\n",
    "    \n",
    "    # def forward(self, tensor):\n",
    "    #     return tensor\n",
    "    \n",
    "class SubtractMean_and_HorizontalFlip:\n",
    "    def __init__(self, root, _dataset_name=\"CIFAR100\"):\n",
    "        self.transform_flip = Compose([\n",
    "            ToTensor(),\n",
    "            RandomHorizontalFlip(p=1.0),\n",
    "        ])\n",
    "        self.transform_default = Compose([\n",
    "            ToTensor(),\n",
    "            Submean(),\n",
    "        ])\n",
    "        if _dataset_name == \"CIFAR100\":\n",
    "            self.ref_training_data = datasets.CIFAR100(\n",
    "                root=root,\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=self.transform_default,\n",
    "            )\n",
    "            self.ref_test_data = datasets.CIFAR100(\n",
    "                root=root,\n",
    "                train=False,\n",
    "                download=True,\n",
    "                # transform=self.transform_default,\n",
    "                transform=ToTensor(),\n",
    "                \n",
    "            )\n",
    "            \n",
    "        # Make copies\n",
    "        self.training_data1 = self.ref_training_data\n",
    "        self.training_data2 = self.ref_training_data\n",
    "        \n",
    "        # Apply Flip transform\n",
    "        self.training_data2.transform = self.transform_flip\n",
    "\n",
    "        # Merge datasets\n",
    "        self.training_data = ConcatDataset([self.training_data1, self.training_data2])\n",
    "        \n",
    "        # Copy classes data\n",
    "        self.classes = self.ref_training_data.classes\n",
    "        self.training_data.classes = self.classes\n",
    "        \n",
    "        self.class_to_idx = self.ref_training_data.class_to_idx\n",
    "        self.training_data.class_to_idx = self.class_to_idx\n",
    "        \n",
    "        return \n",
    "\n",
    "    def unpack(self):\n",
    "        print(\"SubtractMean_and_HorizontalFlip\")\n",
    "        return self.training_data, self.ref_test_data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "subtract mean 적용 한 것.\n",
    "\n",
    "아니 왜 training set에서는 sub mean이 잘 되는데, test set에서는 sub mean하면 안되는거야?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Usage\n",
    "batch_size = 256\n",
    "_dataset = SubtractMean_and_HorizontalFlip('data', 'CIFAR100')\n",
    "training_data, test_data = _dataset.unpack()\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize ResNet34 model\n",
    "model = resnet34(pretrained=False, num_classes=100).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:\n",
      "Train Loss: 3.6400 | Train Acc: 14.93%\n",
      "Test Loss: 3.4300 | Test Acc: 18.37%\n",
      "--------------------------------------------------\n",
      "Epoch 2/20:\n",
      "Train Loss: 2.8043 | Train Acc: 29.19%\n",
      "Test Loss: 3.2005 | Test Acc: 24.06%\n",
      "--------------------------------------------------\n",
      "Epoch 3/20:\n",
      "Train Loss: 2.2785 | Train Acc: 39.97%\n",
      "Test Loss: 2.9146 | Test Acc: 30.39%\n",
      "--------------------------------------------------\n",
      "Epoch 4/20:\n",
      "Train Loss: 1.8403 | Train Acc: 49.77%\n",
      "Test Loss: 3.1405 | Test Acc: 29.15%\n",
      "--------------------------------------------------\n",
      "Epoch 5/20:\n",
      "Train Loss: 1.4305 | Train Acc: 59.51%\n",
      "Test Loss: 3.0664 | Test Acc: 32.10%\n",
      "--------------------------------------------------\n",
      "Epoch 6/20:\n",
      "Train Loss: 1.0630 | Train Acc: 68.83%\n",
      "Test Loss: 3.5527 | Test Acc: 30.13%\n",
      "--------------------------------------------------\n",
      "Epoch 7/20:\n",
      "Train Loss: 0.7578 | Train Acc: 76.95%\n",
      "Test Loss: 3.6140 | Test Acc: 31.51%\n",
      "--------------------------------------------------\n",
      "Epoch 8/20:\n",
      "Train Loss: 0.5140 | Train Acc: 83.88%\n",
      "Test Loss: 3.8856 | Test Acc: 32.52%\n",
      "--------------------------------------------------\n",
      "Epoch 9/20:\n",
      "Train Loss: 0.3284 | Train Acc: 89.62%\n",
      "Test Loss: 4.2622 | Test Acc: 32.24%\n",
      "--------------------------------------------------\n",
      "Epoch 10/20:\n",
      "Train Loss: 0.1995 | Train Acc: 93.80%\n",
      "Test Loss: 4.3277 | Test Acc: 33.02%\n",
      "--------------------------------------------------\n",
      "Epoch 11/20:\n",
      "Train Loss: 0.1254 | Train Acc: 96.26%\n",
      "Test Loss: 4.3955 | Test Acc: 34.57%\n",
      "--------------------------------------------------\n",
      "Epoch 12/20:\n",
      "Train Loss: 0.0606 | Train Acc: 98.36%\n",
      "Test Loss: 4.5247 | Test Acc: 34.94%\n",
      "--------------------------------------------------\n",
      "Epoch 13/20:\n",
      "Train Loss: 0.0206 | Train Acc: 99.59%\n",
      "Test Loss: 4.4166 | Test Acc: 36.83%\n",
      "--------------------------------------------------\n",
      "Epoch 14/20:\n",
      "Train Loss: 0.0061 | Train Acc: 99.93%\n",
      "Test Loss: 4.3428 | Test Acc: 37.33%\n",
      "--------------------------------------------------\n",
      "Epoch 15/20:\n",
      "Train Loss: 0.0037 | Train Acc: 99.94%\n",
      "Test Loss: 4.3752 | Test Acc: 37.55%\n",
      "--------------------------------------------------\n",
      "Epoch 16/20:\n",
      "Train Loss: 0.0026 | Train Acc: 99.94%\n",
      "Test Loss: 4.3582 | Test Acc: 38.05%\n",
      "--------------------------------------------------\n",
      "Epoch 17/20:\n",
      "Train Loss: 0.0027 | Train Acc: 99.94%\n",
      "Test Loss: 4.3243 | Test Acc: 37.59%\n",
      "--------------------------------------------------\n",
      "Epoch 18/20:\n",
      "Train Loss: 0.0021 | Train Acc: 99.95%\n",
      "Test Loss: 4.3282 | Test Acc: 37.61%\n",
      "--------------------------------------------------\n",
      "Epoch 19/20:\n",
      "Train Loss: 0.0018 | Train Acc: 99.96%\n",
      "Test Loss: 4.3345 | Test Acc: 37.71%\n",
      "--------------------------------------------------\n",
      "Epoch 20/20:\n",
      "Train Loss: 0.0018 | Train Acc: 99.96%\n",
      "Test Loss: 4.2761 | Test Acc: 37.80%\n",
      "--------------------------------------------------\n",
      "Trained model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc = correct / total\n",
    "\n",
    "    # Print epoch statistics\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"resnet34_cifar100.pth\")\n",
    "print(\"Trained model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_modules of ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=100, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
