{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "hello world\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")\n",
    "print(\"hello world\")\n",
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 3.]\n",
      " [4. 4.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "y = tf.constant([[0, 1], [1, 0]], dtype=tf.float32)\n",
    "z = x+y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.ones(shape=[3,2], dtype=tf.float32)\n",
    "x2 = tf.random.normal(shape=(10, 2), mean=0.0, stddev=1.0, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.240001  15.32      -7.44     ]\n",
      " [15.32      27.009998  -6.67     ]\n",
      " [-7.44      -6.67       6.8900003]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[3.2, -1], [5.1, 1], [-1.7, 2.0]])\n",
    "y = tf.transpose(x)\n",
    "z1 = x@y\n",
    "print(z1.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2 = tf.nn.softmax(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.6626373e-02 9.8337364e-01 1.2828506e-10]\n",
      " [8.3771147e-06 9.9999166e-01 2.3602503e-15]\n",
      " [5.9780467e-07 1.2911177e-06 9.9999809e-01]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[3., 3., 3.],\n",
      "       [4., 5., 6.]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[4., 4., 4.],\n",
      "       [5., 6., 7.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "v1 = tf.Variable([[1, 2, 3], [4, 5, 6]], dtype=tf.float32)\n",
    "v2 = tf.Variable([[1, 2, 3], [4, 5, 6]], dtype=tf.float32)\n",
    "v1[0].assign([3, 3, 3])\n",
    "print(v1)\n",
    "v1.assign_add([[1, 1, 1], [1, 1, 1]])\n",
    "print(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x ** 2\n",
    "    \n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(y)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, cost: 12.5, w: 0.5\n",
      "step: 1, cost: 10.125, w: 0.9500000476837158\n",
      "step: 2, cost: 8.201250076293945, w: 1.3550000190734863\n",
      "step: 3, cost: 6.643012523651123, w: 1.7195000648498535\n",
      "step: 4, cost: 5.380840301513672, w: 2.0475499629974365\n",
      "step: 5, cost: 4.358480453491211, w: 2.342794895172119\n",
      "step: 6, cost: 3.530369758605957, w: 2.608515501022339\n",
      "step: 7, cost: 2.8595991134643555, w: 2.8476638793945312\n",
      "step: 8, cost: 2.3162755966186523, w: 3.0628974437713623\n",
      "step: 9, cost: 1.8761825561523438, w: 3.2566077709198\n",
      "step: 10, cost: 1.5197086334228516, w: 3.4309470653533936\n",
      "step: 11, cost: 1.2309627532958984, w: 3.5878524780273438\n",
      "step: 12, cost: 0.9970798492431641, w: 3.729067325592041\n",
      "step: 13, cost: 0.8076353073120117, w: 3.8561606407165527\n",
      "step: 14, cost: 0.6541843414306641, w: 3.9705445766448975\n",
      "step: 15, cost: 0.5298900604248047, w: 4.073490142822266\n",
      "step: 16, cost: 0.4292106628417969, w: 4.166141033172607\n",
      "step: 17, cost: 0.3476600646972656, w: 4.2495269775390625\n",
      "step: 18, cost: 0.2816047668457031, w: 4.3245744705200195\n",
      "step: 19, cost: 0.22810077667236328, w: 4.392117023468018\n",
      "step: 20, cost: 0.18476104736328125, w: 4.452905178070068\n",
      "step: 21, cost: 0.1496562957763672, w: 4.507614612579346\n",
      "step: 22, cost: 0.12122249603271484, w: 4.556853294372559\n",
      "step: 23, cost: 0.09818840026855469, w: 4.601168155670166\n",
      "step: 24, cost: 0.07953357696533203, w: 4.641051292419434\n",
      "step: 25, cost: 0.06442070007324219, w: 4.67694616317749\n",
      "step: 26, cost: 0.052181243896484375, w: 4.709251403808594\n",
      "step: 27, cost: 0.042267799377441406, w: 4.738326072692871\n",
      "step: 28, cost: 0.03423595428466797, w: 4.764493465423584\n",
      "step: 29, cost: 0.027730941772460938, w: 4.788043975830078\n",
      "step: 30, cost: 0.022462844848632812, w: 4.809239387512207\n",
      "step: 31, cost: 0.01819610595703125, w: 4.828315258026123\n",
      "step: 32, cost: 0.014737129211425781, w: 4.845483779907227\n",
      "step: 33, cost: 0.011938095092773438, w: 4.860935211181641\n",
      "step: 34, cost: 0.009669303894042969, w: 4.874841690063477\n",
      "step: 35, cost: 0.007832527160644531, w: 4.887357711791992\n",
      "step: 36, cost: 0.006343841552734375, w: 4.898622035980225\n",
      "step: 37, cost: 0.005138397216796875, w: 4.908760070800781\n",
      "step: 38, cost: 0.004162788391113281, w: 4.91788387298584\n",
      "step: 39, cost: 0.0033702850341796875, w: 4.926095485687256\n",
      "step: 40, cost: 0.0027303695678710938, w: 4.933485984802246\n",
      "step: 41, cost: 0.0022115707397460938, w: 4.9401373863220215\n",
      "step: 42, cost: 0.0017910003662109375, w: 4.9461236000061035\n",
      "step: 43, cost: 0.0014514923095703125, w: 4.951511383056641\n",
      "step: 44, cost: 0.0011758804321289062, w: 4.956360340118408\n",
      "step: 45, cost: 0.0009527206420898438, w: 4.960724353790283\n",
      "step: 46, cost: 0.0007715225219726562, w: 4.964652061462402\n",
      "step: 47, cost: 0.0006237030029296875, w: 4.968186855316162\n",
      "step: 48, cost: 0.0005054473876953125, w: 4.971368312835693\n",
      "step: 49, cost: 0.00040912628173828125, w: 4.974231719970703\n",
      "step: 50, cost: 0.000331878662109375, w: 4.976808547973633\n",
      "step: 51, cost: 0.0002689361572265625, w: 4.979127883911133\n",
      "step: 52, cost: 0.000217437744140625, w: 4.981215000152588\n",
      "step: 53, cost: 0.0001773834228515625, w: 4.98309326171875\n",
      "step: 54, cost: 0.0001430511474609375, w: 4.984784126281738\n",
      "step: 55, cost: 0.0001163482666015625, w: 4.9863057136535645\n",
      "step: 56, cost: 9.441375732421875e-05, w: 4.987675189971924\n",
      "step: 57, cost: 7.62939453125e-05, w: 4.988907814025879\n",
      "step: 58, cost: 6.29425048828125e-05, w: 4.990016937255859\n",
      "step: 59, cost: 4.9591064453125e-05, w: 4.991015434265137\n",
      "step: 60, cost: 3.910064697265625e-05, w: 4.991913795471191\n",
      "step: 61, cost: 3.337860107421875e-05, w: 4.992722511291504\n",
      "step: 62, cost: 2.765655517578125e-05, w: 4.993450164794922\n",
      "step: 63, cost: 2.09808349609375e-05, w: 4.994105339050293\n",
      "step: 64, cost: 1.621246337890625e-05, w: 4.994694709777832\n",
      "step: 65, cost: 1.52587890625e-05, w: 4.995225429534912\n",
      "step: 66, cost: 1.049041748046875e-05, w: 4.995702743530273\n",
      "step: 67, cost: 9.5367431640625e-06, w: 4.9961323738098145\n",
      "step: 68, cost: 7.62939453125e-06, w: 4.996519088745117\n",
      "step: 69, cost: 5.7220458984375e-06, w: 4.9968671798706055\n",
      "step: 70, cost: 3.814697265625e-06, w: 4.997180461883545\n",
      "step: 71, cost: 3.814697265625e-06, w: 4.997462272644043\n",
      "step: 72, cost: 1.9073486328125e-06, w: 4.997715950012207\n",
      "step: 73, cost: 3.814697265625e-06, w: 4.997944355010986\n",
      "step: 74, cost: 2.86102294921875e-06, w: 4.998149871826172\n",
      "step: 75, cost: 1.9073486328125e-06, w: 4.998334884643555\n",
      "step: 76, cost: 9.5367431640625e-07, w: 4.998501300811768\n",
      "step: 77, cost: 1.9073486328125e-06, w: 4.998651027679443\n",
      "step: 78, cost: 0.0, w: 4.998785972595215\n",
      "step: 79, cost: 0.0, w: 4.998907566070557\n",
      "step: 80, cost: 9.5367431640625e-07, w: 4.999016761779785\n",
      "step: 81, cost: 1.9073486328125e-06, w: 4.999114990234375\n",
      "step: 82, cost: 0.0, w: 4.999203681945801\n",
      "step: 83, cost: 9.5367431640625e-07, w: 4.999283313751221\n",
      "step: 84, cost: 9.5367431640625e-07, w: 4.999354839324951\n",
      "step: 85, cost: 0.0, w: 4.999419212341309\n",
      "step: 86, cost: -9.5367431640625e-07, w: 4.999477386474609\n",
      "step: 87, cost: 0.0, w: 4.999529838562012\n",
      "step: 88, cost: -9.5367431640625e-07, w: 4.999577045440674\n",
      "step: 89, cost: 9.5367431640625e-07, w: 4.999619483947754\n",
      "step: 90, cost: 9.5367431640625e-07, w: 4.99965763092041\n",
      "step: 91, cost: 9.5367431640625e-07, w: 4.999691963195801\n",
      "step: 92, cost: 9.5367431640625e-07, w: 4.999722957611084\n",
      "step: 93, cost: 0.0, w: 4.99975061416626\n",
      "step: 94, cost: 9.5367431640625e-07, w: 4.999775409698486\n",
      "step: 95, cost: 9.5367431640625e-07, w: 4.999797821044922\n",
      "step: 96, cost: 0.0, w: 4.999817848205566\n",
      "step: 97, cost: 9.5367431640625e-07, w: 4.999835968017578\n",
      "step: 98, cost: 0.0, w: 4.999852180480957\n",
      "step: 99, cost: 9.5367431640625e-07, w: 4.999866962432861\n",
      "step: 100, cost: 9.5367431640625e-07, w: 4.999880313873291\n",
      "step: 101, cost: 9.5367431640625e-07, w: 4.999892234802246\n",
      "step: 102, cost: -9.5367431640625e-07, w: 4.999903202056885\n",
      "step: 103, cost: 9.5367431640625e-07, w: 4.999912738800049\n",
      "step: 104, cost: 9.5367431640625e-07, w: 4.9999213218688965\n",
      "step: 105, cost: 0.0, w: 4.999929428100586\n",
      "step: 106, cost: 0.0, w: 4.999936580657959\n",
      "step: 107, cost: 0.0, w: 4.999942779541016\n",
      "step: 108, cost: 0.0, w: 4.999948501586914\n",
      "step: 109, cost: 0.0, w: 4.999953746795654\n",
      "step: 110, cost: 0.0, w: 4.999958515167236\n",
      "step: 111, cost: 9.5367431640625e-07, w: 4.99996280670166\n",
      "step: 112, cost: 9.5367431640625e-07, w: 4.999966621398926\n",
      "step: 113, cost: 9.5367431640625e-07, w: 4.999969959259033\n",
      "step: 114, cost: 9.5367431640625e-07, w: 4.999972820281982\n",
      "step: 115, cost: 0.0, w: 4.999975681304932\n",
      "step: 116, cost: 9.5367431640625e-07, w: 4.999978065490723\n",
      "step: 117, cost: 9.5367431640625e-07, w: 4.999980449676514\n",
      "step: 118, cost: 0.0, w: 4.9999823570251465\n",
      "step: 119, cost: 0.0, w: 4.999984264373779\n",
      "step: 120, cost: 0.0, w: 4.999985694885254\n",
      "step: 121, cost: 9.5367431640625e-07, w: 4.9999871253967285\n",
      "step: 122, cost: 9.5367431640625e-07, w: 4.999988555908203\n",
      "step: 123, cost: 0.0, w: 4.9999895095825195\n",
      "step: 124, cost: 9.5367431640625e-07, w: 4.999990463256836\n",
      "step: 125, cost: 0.0, w: 4.999991416931152\n",
      "step: 126, cost: -9.5367431640625e-07, w: 4.999992370605469\n",
      "step: 127, cost: 0.0, w: 4.999993324279785\n",
      "step: 128, cost: 9.5367431640625e-07, w: 4.999993801116943\n",
      "step: 129, cost: 0.0, w: 4.999994277954102\n",
      "step: 130, cost: 0.0, w: 4.99999475479126\n",
      "step: 131, cost: 9.5367431640625e-07, w: 4.999995231628418\n",
      "step: 132, cost: -9.5367431640625e-07, w: 4.999995708465576\n",
      "step: 133, cost: 0.0, w: 4.999996185302734\n",
      "step: 134, cost: 0.0, w: 4.999996662139893\n",
      "step: 135, cost: 9.5367431640625e-07, w: 4.999997138977051\n",
      "step: 136, cost: 9.5367431640625e-07, w: 4.999997615814209\n",
      "step: 137, cost: 0.0, w: 4.999998092651367\n",
      "step: 138, cost: 0.0, w: 4.999998092651367\n",
      "step: 139, cost: 0.0, w: 4.999998092651367\n",
      "step: 140, cost: 0.0, w: 4.999998092651367\n",
      "step: 141, cost: 0.0, w: 4.999998092651367\n",
      "step: 142, cost: 0.0, w: 4.999998092651367\n",
      "step: 143, cost: 0.0, w: 4.999998092651367\n",
      "step: 144, cost: 0.0, w: 4.999998092651367\n",
      "step: 145, cost: 0.0, w: 4.999998092651367\n",
      "step: 146, cost: 0.0, w: 4.999998092651367\n",
      "step: 147, cost: 0.0, w: 4.999998092651367\n",
      "step: 148, cost: 0.0, w: 4.999998092651367\n",
      "step: 149, cost: 0.0, w: 4.999998092651367\n",
      "step: 150, cost: 0.0, w: 4.999998092651367\n",
      "step: 151, cost: 0.0, w: 4.999998092651367\n",
      "step: 152, cost: 0.0, w: 4.999998092651367\n",
      "step: 153, cost: 0.0, w: 4.999998092651367\n",
      "step: 154, cost: 0.0, w: 4.999998092651367\n",
      "step: 155, cost: 0.0, w: 4.999998092651367\n",
      "step: 156, cost: 0.0, w: 4.999998092651367\n",
      "step: 157, cost: 0.0, w: 4.999998092651367\n",
      "step: 158, cost: 0.0, w: 4.999998092651367\n",
      "step: 159, cost: 0.0, w: 4.999998092651367\n",
      "step: 160, cost: 0.0, w: 4.999998092651367\n",
      "step: 161, cost: 0.0, w: 4.999998092651367\n",
      "step: 162, cost: 0.0, w: 4.999998092651367\n",
      "step: 163, cost: 0.0, w: 4.999998092651367\n",
      "step: 164, cost: 0.0, w: 4.999998092651367\n",
      "step: 165, cost: 0.0, w: 4.999998092651367\n",
      "step: 166, cost: 0.0, w: 4.999998092651367\n",
      "step: 167, cost: 0.0, w: 4.999998092651367\n",
      "step: 168, cost: 0.0, w: 4.999998092651367\n",
      "step: 169, cost: 0.0, w: 4.999998092651367\n",
      "step: 170, cost: 0.0, w: 4.999998092651367\n",
      "step: 171, cost: 0.0, w: 4.999998092651367\n",
      "step: 172, cost: 0.0, w: 4.999998092651367\n",
      "step: 173, cost: 0.0, w: 4.999998092651367\n",
      "step: 174, cost: 0.0, w: 4.999998092651367\n",
      "step: 175, cost: 0.0, w: 4.999998092651367\n",
      "step: 176, cost: 0.0, w: 4.999998092651367\n",
      "step: 177, cost: 0.0, w: 4.999998092651367\n",
      "step: 178, cost: 0.0, w: 4.999998092651367\n",
      "step: 179, cost: 0.0, w: 4.999998092651367\n",
      "step: 180, cost: 0.0, w: 4.999998092651367\n",
      "step: 181, cost: 0.0, w: 4.999998092651367\n",
      "step: 182, cost: 0.0, w: 4.999998092651367\n",
      "step: 183, cost: 0.0, w: 4.999998092651367\n",
      "step: 184, cost: 0.0, w: 4.999998092651367\n",
      "step: 185, cost: 0.0, w: 4.999998092651367\n",
      "step: 186, cost: 0.0, w: 4.999998092651367\n",
      "step: 187, cost: 0.0, w: 4.999998092651367\n",
      "step: 188, cost: 0.0, w: 4.999998092651367\n",
      "step: 189, cost: 0.0, w: 4.999998092651367\n",
      "step: 190, cost: 0.0, w: 4.999998092651367\n",
      "step: 191, cost: 0.0, w: 4.999998092651367\n",
      "step: 192, cost: 0.0, w: 4.999998092651367\n",
      "step: 193, cost: 0.0, w: 4.999998092651367\n",
      "step: 194, cost: 0.0, w: 4.999998092651367\n",
      "step: 195, cost: 0.0, w: 4.999998092651367\n",
      "step: 196, cost: 0.0, w: 4.999998092651367\n",
      "step: 197, cost: 0.0, w: 4.999998092651367\n",
      "step: 198, cost: 0.0, w: 4.999998092651367\n",
      "step: 199, cost: 0.0, w: 4.999998092651367\n"
     ]
    }
   ],
   "source": [
    "leanring_rate = 0.1\n",
    "w = tf.Variable(0, dtype=tf.float32)\n",
    "\n",
    "def train_step(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = x * (w**2 - 10*w + 25)\n",
    "    dw = tape.gradient(cost, w)\n",
    "    w.assign_sub(leanring_rate * dw)\n",
    "    return cost\n",
    "\n",
    "for i in range(200):\n",
    "    cost = train_step(0.5)\n",
    "    print(f\"step: {i}, cost: {cost}, w: {w.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
