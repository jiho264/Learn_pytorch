{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from Mymodel import MyResNet34\n",
    "from Mymodel import MyResNet_CIFAR\n",
    "import tqdm\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms.v2 import (\n",
    "    ToTensor,\n",
    "    RandomHorizontalFlip,\n",
    "    Compose,\n",
    "    RandomCrop,\n",
    "    RandomShortestSize,\n",
    "    AutoAugment,\n",
    "    Normalize,\n",
    "    TenCrop,\n",
    "    CenterCrop,\n",
    "    Pad,\n",
    "    Resize,\n",
    "    Lambda\n",
    ")\n",
    "from torchvision.transforms.autoaugment import AutoAugmentPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataset selection\"\"\"\n",
    "# DATASET = \"CIFAR10\"\n",
    "# DATASET = \"CIFAR100\"\n",
    "DATASET = \"ImageNet2012\"\n",
    "\n",
    "\"\"\"Model selection for CIFAR\"\"\"\n",
    "NUM_LAYERS_LEVEL = 5\n",
    "\n",
    "\"\"\"Dataset parameters\"\"\"\n",
    "BATCH = 256\n",
    "SHUFFLE = True\n",
    "NUMOFWORKERS = 8\n",
    "PIN_MEMORY = True\n",
    "SPLIT_RATIO = 0\n",
    "\n",
    "\"\"\"optimizer parameters\"\"\"\n",
    "OPTIMIZER = \"SGD\"\n",
    "# OPTIMIZER = \"Adam\"\n",
    "# OPTIMIZER = \"Adam_decay\"\n",
    "\n",
    "\"\"\"Learning rate scheduler parameters\"\"\"\n",
    "# LOAD_BEFORE_TRAINING = False\n",
    "LOAD_BEFORE_TRAINING = True\n",
    "NUM_EPOCHS = 1000\n",
    "scheduler_patience_mapping = {\"CIFAR10\": 100, \"CIFAR100\": 100, \"ImageNet2012\": 5}\n",
    "\n",
    "\"\"\"Early stopping parameters\"\"\"\n",
    "EARLYSTOPPINGPATIENCE = 25\n",
    "file_path = \"\"\n",
    "if DATASET == \"ImageNet2012\":\n",
    "    file_path = f\"{DATASET}/MyResNet34_{BATCH}_{OPTIMIZER}\"\n",
    "else:\n",
    "    file_path = f\"{DATASET}/MyResNet{NUM_LAYERS_LEVEL*6+2}_{BATCH}_{OPTIMIZER}\"\n",
    "\n",
    "if SPLIT_RATIO != 0:\n",
    "    file_path += f\"_{int(SPLIT_RATIO*100)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataset:\n",
    "    def __init__(self, root, seceted_dataset, split_ratio=0):\n",
    "        self.Randp = 0.5\n",
    "        self.dataset_name = seceted_dataset\n",
    "        self.split_ratio = split_ratio\n",
    "\n",
    "        if self.dataset_name[:5] == \"CIFAR\":\n",
    "            pass\n",
    "        elif self.dataset_name == \"ImageNet2012\":\n",
    "            self.ImageNetRoot = root + \"/\" + self.dataset_name + \"/\"\n",
    "\n",
    "            \"\"\"\n",
    "            각 지정된 스케일에 따라 10 crop해야하는데, 5개 scale들의 평균을 내야하니까 좀 번거로움.\n",
    "            그치만, 학습 중엔 center crop으로 eval하니, 지금 당장 필요하지는 않음.\n",
    "            \"\"\"\n",
    "\n",
    "            test_data_list = list()\n",
    "            scales = [224, 256, 384, 480, 640]\n",
    "            for scale in scales:\n",
    "                # test_data_list.append(\n",
    "                #     datasets.ImageFolder(\n",
    "                #         root=self.ImageNetRoot + \"val\",\n",
    "                #         transform=Compose(\n",
    "                #             [\n",
    "                #                 # RandomShortestSize(min_size=scale+1, antialias=True),\n",
    "                #                 RandomShortestSize(min_size=scale+1, antialias=True),\n",
    "                #                 # Resize(size=[scale, scale]),\n",
    "                #                 TenCrop(size=scale),\n",
    "                #                 ToTensor(),\n",
    "                #                 Normalize(\n",
    "                #                     mean=[0.485, 0.456, 0.406], std=[1, 1, 1], inplace=True\n",
    "                #                 ),\n",
    "                #             ]\n",
    "                #         ),\n",
    "                #     )\n",
    "                # )\n",
    "                test_data_list.append(\n",
    "                    datasets.CIFAR100(\n",
    "                        root=root,\n",
    "                        train=True,\n",
    "                        download=False,\n",
    "                        transform=Compose(\n",
    "                            [\n",
    "                                # RandomShortestSize(min_size=scale+1, antialias=True),\n",
    "                                # RandomShortestSize(min_size=scale+1, antialias=True),\n",
    "                                # Resize(size=[scale, scale]),\n",
    "                                ToTensor(),\n",
    "                                TenCrop(size=20),\n",
    "                                # Lambda(lambda crops: torch.stack(crops)),\n",
    "                            ]\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "            self.test_data_list = test_data_list\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dataset: {self.dataset_name}\")\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lee/anaconda3/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tmp = LoadDataset(root=\"../data\", seceted_dataset=DATASET)\n",
    "COUNT_OF_CLASSES = 1000\n",
    "test_data = tmp.test_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR100\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 ToTensor()\n",
      "                 TenCrop(size=(20, 20), vertical_flip=False)\n",
      "           )\n",
      "Dataset CIFAR100\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 ToTensor()\n",
      "                 TenCrop(size=(20, 20), vertical_flip=False)\n",
      "           )\n",
      "Dataset CIFAR100\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 ToTensor()\n",
      "                 TenCrop(size=(20, 20), vertical_flip=False)\n",
      "           )\n",
      "Dataset CIFAR100\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 ToTensor()\n",
      "                 TenCrop(size=(20, 20), vertical_flip=False)\n",
      "           )\n",
      "Dataset CIFAR100\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 ToTensor()\n",
      "                 TenCrop(size=(20, 20), vertical_flip=False)\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(test_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader_list = list()\n",
    "\n",
    "for i in range(5):\n",
    "    test_dataloader_list.append(\n",
    "        DataLoader(\n",
    "            test_data[i],\n",
    "            batch_size=BATCH,\n",
    "            shuffle=SHUFFLE,\n",
    "            # num_workers=NUMOFWORKERS,\n",
    "            # pin_memory=PIN_MEMORY,\n",
    "            # pin_memory_device=\"cuda\",\n",
    "            # persistent_workers=True,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR100\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 ToTensor()\n",
      "                 TenCrop(size=(20, 20), vertical_flip=False)\n",
      "           )\n",
      "Dataset CIFAR100\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 ToTensor()\n",
      "                 TenCrop(size=(20, 20), vertical_flip=False)\n",
      "           )\n",
      "Dataset CIFAR100\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 ToTensor()\n",
      "                 TenCrop(size=(20, 20), vertical_flip=False)\n",
      "           )\n",
      "Dataset CIFAR100\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 ToTensor()\n",
      "                 TenCrop(size=(20, 20), vertical_flip=False)\n",
      "           )\n",
      "Dataset CIFAR100\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 ToTensor()\n",
      "                 TenCrop(size=(20, 20), vertical_flip=False)\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(test_dataloader_list[i].dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-34 for ImageNet2012 is loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if DATASET == \"ImageNet2012\":\n",
    "    model = MyResNet34(\n",
    "    num_classes=COUNT_OF_CLASSES, \n",
    "    Downsample_option=\"B\"\n",
    "    ).to(device)\n",
    "    # model = models.resnet34(pretrained=True).to(device)\n",
    "    # model = models.resnet34(pretrained=False).to(device)\n",
    "    print(f\"ResNet-34 for {DATASET} is loaded.\")\n",
    "else:\n",
    "    model = MyResNet_CIFAR(\n",
    "        num_classes=COUNT_OF_CLASSES, num_layer_factor=NUM_LAYERS_LEVEL\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(f\"../models/{file_path}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test4:   0%|          | 0/196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "torch.Size([256, 3, 20, 20])\n",
      "torch.Size([256, 3, 20, 20])\n",
      "torch.Size([256, 3, 20, 20])\n",
      "torch.Size([256, 3, 20, 20])\n",
      "torch.Size([256, 3, 20, 20])\n",
      "torch.Size([256, 3, 20, 20])\n",
      "torch.Size([256, 3, 20, 20])\n",
      "torch.Size([256, 3, 20, 20])\n",
      "torch.Size([256, 3, 20, 20])\n",
      "torch.Size([256, 3, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for images, labels in tqdm.tqdm(test_dataloader_list[i], desc=f\"test{i}\"):\n",
    "    print(\"-----------------------------------------\")\n",
    "    for img in images:\n",
    "        print(img.shape)\n",
    "    # print(len(images), len(labels))\n",
    "    # print(images[0].shape, labels.shape)\n",
    "    # for img in images:\n",
    "    #     print(img.shape)\n",
    "    #     img.to(device)\n",
    "    # for label in labels:\n",
    "    #     print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test0:  47%|████▋     | 93/196 [00:15<00:13,  7.56it/s]"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "avg_loss = 0\n",
    "avg_acc = 0\n",
    "for i in range(5):\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm.tqdm(test_dataloader_list[i], desc=f\"test{i}\"):\n",
    "            for img in images:\n",
    "                img, labels = img.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(img)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        test_loss /= len(test_dataloader_list[i])\n",
    "        test_acc = correct / total\n",
    "        print(test_loss, test_acc)\n",
    "        avg_loss += test_loss\n",
    "        avg_acc += test_acc\n",
    "\n",
    "print(avg_loss / 5, avg_acc / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test_loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"test_acc: {test_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"test_error: {100 - test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
